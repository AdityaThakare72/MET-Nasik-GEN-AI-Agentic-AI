{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Egxpx5ZlVSNR"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'evaluate'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-1809916668\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeteor_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeteor_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# Install required packages\n","!pip install transformers evaluate nltk rouge-score --quiet\n","\n","# Imports\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","import evaluate\n","from nltk.translate.meteor_score import meteor_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKpcwkb2Ve3_"},"outputs":[],"source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-FLRn73VgkM"},"outputs":[],"source":["# generate the sentence\n","\n","prompt = \"The future of artificial intelligence is\"\n","\n","input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","# Generate text with top-p sampling\n","output_ids = model.generate(\n","    input_ids=input_ids,\n","    max_length=30,\n","    do_sample=True,\n","    top_p=0.9,\n","    top_k=50,\n","    temperature=0.8\n",")\n","\n","generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","print(\"üß† Generated Output:\\n\", generated_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsP7QgX5VlBq"},"outputs":[],"source":["# reference sentence\n","\n","# Reference sentences written manually\n","references = [\n","    \"The future of artificial intelligence is full of potential and ethical challenges.\",\n","    \"Artificial intelligence will transform industries in the future.\"\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GESMofbYVoks"},"outputs":[],"source":["# BLEU\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","candidate_tokens = word_tokenize(generated_text.lower())\n","reference_tokens = [word_tokenize(ref.lower()) for ref in references]\n","\n","bleu_score = sentence_bleu(reference_tokens, candidate_tokens,\n","                           smoothing_function=SmoothingFunction().method1)\n","print(\"üéØ BLEU Score:\", round(bleu_score, 4))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHQ9D8Y8VrOk"},"outputs":[],"source":["# Rouge\n","\n","from rouge_score import rouge_scorer\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# ROUGE requires string format\n","for ref in references:\n","    rouge_scores = scorer.score(ref, generated_text)\n","    print(f\"\\nüîç ROUGE Scores vs: {ref}\")\n","    for key, value in rouge_scores.items():\n","        print(f\"{key}: Precision={value.precision:.4f}, Recall={value.recall:.4f}, F1={value.fmeasure:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEx7in2DVvqb"},"outputs":[],"source":["# meteor\n","\n","meteor_scores = [meteor_score([ref], generated_text) for ref in references]\n","print(\"\\nüìê METEOR Scores:\")\n","for i, score in enumerate(meteor_scores):\n","    print(f\"Reference {i+1}: {round(score, 4)}\")\n","\n","# Average Meteor score\n","print(\"üìä Avg METEOR:\", round(sum(meteor_scores)/len(meteor_scores), 4))\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPKLGGRQr515k5rgTSSm7HK","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}