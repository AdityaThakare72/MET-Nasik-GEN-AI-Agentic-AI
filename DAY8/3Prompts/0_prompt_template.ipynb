{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27d8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Because templates are reusable blueprints. They allow you to swap models, change formatting, or share \"prompt logic\" across your entire application without rewriting strings\n",
    "\n",
    "Template Type       Best For                            Output\n",
    "PromptTemplate      Completion Models (LLMs)            A single String.\n",
    "ChatPromptTemplate  Conversational Models (ChatModels)  A list of Message Objects.\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b072c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/GEN_AI_GOOGLE_NEW/venv_google_langchain_new/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a professional Data Science mentor.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the concept of Gradient Descent in simple terms.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Define the blueprint with placeholders in curly braces\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a professional {role} mentor.\"),\n",
    "    (\"human\", \"Explain the concept of {topic} in simple terms.\")\n",
    "])\n",
    "\n",
    "# 2. Format the template with actual data\n",
    "# This creates a 'PromptValue' object\n",
    "formatted_prompt = template.invoke({\n",
    "    \"role\": \"Data Science\", \n",
    "    \"topic\": \"Gradient Descent\"\n",
    "})\n",
    "\n",
    "# 3. View what goes to the model\n",
    "print(formatted_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b5e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Formatting\n",
    "# Sometimes you know the role of your agent at the start of your program, but you don't know the topic yet. Instead of passing both every time, you can \"pre-fill\" a part of the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68294080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful Python assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are decorators?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "base_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {expert_type} assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Pre-filling the expert_type\n",
    "python_expert_prompt = base_template.partial(expert_type=\"Python\")\n",
    "\n",
    "# Now you only need to provide the question\n",
    "final_prompt = python_expert_prompt.invoke({\"question\": \"What are decorators?\"})\n",
    "\n",
    "print(final_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5222951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful Python assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are decorators?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "base_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {expert_type} assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Pre-filling the expert_type\n",
    "# ython_expert_prompt = base_template.partial(expert_type=\"Python\")\n",
    "\n",
    "# Now you only need to provide the question\n",
    "final_prompt = python_expert_prompt.invoke({\"expert_type\": \"Python\", \"question\": \"What are decorators?\"})\n",
    "\n",
    "print(final_prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f362468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question:\n",
    "\n",
    "Create a ChatPromptTemplate for a \"Translator\" persona.\n",
    "\n",
    "It should take two variables: input_language and text.\n",
    "\n",
    "The system message should say: \"You are an expert translator from {input_language} to English.\"\n",
    "\n",
    "Invoke it with \"Marathi\" and a sentence of your choice.\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649e996d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Prompt Messages ---\n",
      "Role: SYSTEM\n",
      "Content: You are an expert translator from Marathi to English. Provide only the translation.\n",
      "---------------\n",
      "Role: HUMAN\n",
      "Content: Translate this text: आपण लँगचेन शिकत आहोत.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Define the template with placeholders\n",
    "# We use a list of tuples to represent (Role, Content)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert translator from {input_language} to English. Provide only the translation.\"),\n",
    "    (\"human\", \"Translate this text: {text}\")\n",
    "])\n",
    "\n",
    "# 2. Invoke the template to fill the variables\n",
    "# This returns a ChatPromptValue object\n",
    "formatted_prompt = template.invoke({\n",
    "    \"input_language\": \"Marathi\",\n",
    "    \"text\": \"आपण लँगचेन शिकत आहोत.\"\n",
    "})\n",
    "\n",
    "# 3. Printing the resulting message objects\n",
    "# This shows exactly what would be sent to the LLM\n",
    "print(\"--- Final Prompt Messages ---\")\n",
    "for message in formatted_prompt.to_messages():\n",
    "    print(f\"Role: {message.type.upper()}\")\n",
    "    print(f\"Content: {message.content}\")\n",
    "    print(\"-\" * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778a9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using model to get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a76d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gemini Translation ---\n",
      "We are learning LangChain.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Load your GOOGLE_API_KEY from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Initialize Gemini 2.5 Flash Lite\n",
    "# temperature 0.7 gives a good balance of creativity for translation\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=512\n",
    ")\n",
    "\n",
    "# 3. Define our blueprint\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert translator from {input_language} to English. Provide only the translation.\"),\n",
    "    (\"human\", \"Translate this text: {text}\")\n",
    "])\n",
    "\n",
    "# 4. Construct the Chain (LCEL Pipe)\n",
    "chain = template | llm\n",
    "\n",
    "# 5. Invoke and see the wisdom of Gemini\n",
    "result = chain.invoke({\n",
    "    \"input_language\": \"Marathi\",\n",
    "    \"text\": \"आपण लँगचेन शिकत आहोत.\"\n",
    "})\n",
    "\n",
    "print(\"--- Gemini Translation ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786f019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_google_langchain_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
